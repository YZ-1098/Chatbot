{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fdcc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95e01ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa4921",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "except Exception:\n",
    "    corpus_bleu = None  # type: ignore\n",
    "    SmoothingFunction = None  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ce081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from rouge_score import rouge_scorer\n",
    "except Exception:\n",
    "    rouge_scorer = None  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f359b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conversation_csv(csv_path: str) -> Tuple[List[str], List[str]]:\n",
    "    questions: List[str] = []\n",
    "    answers: List[str] = []\n",
    "    with open(csv_path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            q = (row.get('question') or '').strip()\n",
    "            a = (row.get('answer') or '').strip()\n",
    "            if not q or not a:\n",
    "                continue\n",
    "            questions.append(q)\n",
    "            answers.append(a)\n",
    "    if not questions:\n",
    "        raise ValueError(\"No question/answer rows found in Conversation.csv\")\n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f19010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_artifacts(base_dir: str):\n",
    "    with open(os.path.join(base_dir, \"tfidf_vectorizer.pkl\"), \"rb\") as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "    with open(os.path.join(base_dir, \"qa_matrix.pkl\"), \"rb\") as f:\n",
    "        qa_matrix = pickle.load(f)\n",
    "    with open(os.path.join(base_dir, \"qa_answers.pkl\"), \"rb\") as f:\n",
    "        answers = pickle.load(f)\n",
    "    return vectorizer, qa_matrix, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa23f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(\n",
    "    questions: List[str],\n",
    "    gold_answers: List[str],\n",
    "    vectorizer,\n",
    "    qa_matrix,\n",
    "    index_to_answer: List[str],\n",
    "    threshold: float = 0.35,\n",
    "):\n",
    "    y_true: List[int] = []\n",
    "    y_pred: List[int] = []\n",
    "\n",
    "    # Gold answer to index mapping (first occurrence wins)\n",
    "    answer_to_index = {}\n",
    "    for idx, ans in enumerate(index_to_answer):\n",
    "        answer_to_index.setdefault(ans, idx)\n",
    "\n",
    "    for q, gold in zip(questions, gold_answers):\n",
    "        user_vec = vectorizer.transform([q])\n",
    "        sims = cosine_similarity(user_vec, qa_matrix).flatten()\n",
    "        best_idx = int(np.argmax(sims))\n",
    "        best_score = float(sims[best_idx])\n",
    "        pred_idx = best_idx if best_score >= threshold else -1\n",
    "\n",
    "        true_idx = answer_to_index.get(gold, -1)\n",
    "        y_true.append(true_idx)\n",
    "        y_pred.append(pred_idx)\n",
    "\n",
    "    # Convert to binary correctness for strict exact-match on answer text\n",
    "    y_true_bin = [1 if t != -1 else 0 for t in y_true]\n",
    "    y_pred_bin = [1 if (p != -1 and index_to_answer[p] == gold_answers[i]) else 0 for i, p in enumerate(y_pred)]\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true_bin, y_pred_bin, average='binary', zero_division=0\n",
    "    )\n",
    "    return {\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1\": float(f1),\n",
    "        \"support\": int(sum(y_true_bin)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generation(\n",
    "    questions: List[str],\n",
    "    gold_answers: List[str],\n",
    "    vectorizer,\n",
    "    qa_matrix,\n",
    "    index_to_answer: List[str],\n",
    "):\n",
    "    # Build predictions (top-1)\n",
    "    preds: List[str] = []\n",
    "    for q in questions:\n",
    "        sims = cosine_similarity(vectorizer.transform([q]), qa_matrix).flatten()\n",
    "        best_idx = int(np.argmax(sims))\n",
    "        preds.append(index_to_answer[best_idx])\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # BLEU (requires nltk)\n",
    "    if corpus_bleu is not None:\n",
    "        smoothie = SmoothingFunction().method3 if SmoothingFunction else None\n",
    "        # corpus_bleu expects references as list of list of tokens; hypotheses as list of tokens\n",
    "        references = [[g.split()] for g in gold_answers]\n",
    "        hypotheses = [p.split() for p in preds]\n",
    "        try:\n",
    "            bleu = corpus_bleu(references, hypotheses, smoothing_function=smoothie)\n",
    "            results[\"bleu\"] = float(bleu)\n",
    "        except Exception:\n",
    "            results[\"bleu\"] = None\n",
    "    else:\n",
    "        results[\"bleu\"] = None\n",
    "\n",
    "    # ROUGE (requires rouge-score)\n",
    "    if rouge_scorer is not None:\n",
    "        scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "        r1_f = []\n",
    "        r2_f = []\n",
    "        rl_f = []\n",
    "        for ref, hyp in zip(gold_answers, preds):\n",
    "            scores = scorer.score(ref, hyp)\n",
    "            r1_f.append(scores[\"rouge1\"].fmeasure)\n",
    "            r2_f.append(scores[\"rouge2\"].fmeasure)\n",
    "            rl_f.append(scores[\"rougeL\"].fmeasure)\n",
    "        results[\"rouge1_f\"] = float(np.mean(r1_f)) if r1_f else None\n",
    "        results[\"rouge2_f\"] = float(np.mean(r2_f)) if r2_f else None\n",
    "        results[\"rougeL_f\"] = float(np.mean(rl_f)) if rl_f else None\n",
    "    else:\n",
    "        results[\"rouge1_f\"] = None\n",
    "        results[\"rouge2_f\"] = None\n",
    "        results[\"rougeL_f\"] = None\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    base_dir = os.path.dirname(__file__)\n",
    "    csv_path = os.path.join(base_dir, \"Conversation.csv\")\n",
    "    questions, gold_answers = read_conversation_csv(csv_path)\n",
    "\n",
    "    vectorizer, qa_matrix, answers = load_artifacts(base_dir)\n",
    "\n",
    "    cls_metrics = evaluate_classification(questions, gold_answers, vectorizer, qa_matrix, answers)\n",
    "    gen_metrics = evaluate_generation(questions, gold_answers, vectorizer, qa_matrix, answers)\n",
    "\n",
    "    print(\"Classification (exact-match on answer text):\")\n",
    "    print(cls_metrics)\n",
    "    print(\"\\nGeneration metrics:\")\n",
    "    print(gen_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab680ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8a1d11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
